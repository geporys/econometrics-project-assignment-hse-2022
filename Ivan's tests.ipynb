{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb37a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white, het_breuschpagan, het_goldfeldquandt, linear_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transactions-2023-01-11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2618e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excess_columns(data):\n",
    "    # Drop high cardinality columns\n",
    "    data = data.drop(columns=[\"Transaction Number\", \"Property ID\", \"Transaction Size (sq.m)\", \"Parking\", \"Project\"])\n",
    "    # Drop low-cardinality columns\n",
    "    data = data.drop(columns=[\"Registration type\", \"Master Project\"])\n",
    "    # Drop leaky columns\n",
    "    data = data.drop(columns=[\"Transaction sub type\", \"Property Type\", \"Room(s)\", \"No. of Buyer\", \"No. of Seller\", 'Nearest Mall', 'Nearest Landmark'])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_excess_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d07a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_oil_price():\n",
    "#     # data from https://tradingeconomics.com/commodity/crude-oil\n",
    "#     r = requests.get('https://markets.tradingeconomics.com/chart?s=cl1:com&interval=1d&span=5y&securify=new&url=/commodity/crude-oil&AUTH=j6Bt4pj3JEXdfgKlsPWa%2BhKpyYuaZeaGdJeVvofKg%2F50KkIli0%2Fe4jDtQXrDXz5r&ohlc=0')\n",
    "    \n",
    "#     oil_data = pd.DataFrame(r.json()['series'][0]['data']).rename(columns={\"y\": \"Oil Price\"})\n",
    "#     oil_data = oil_data[(oil_data['date'] > '2021-03-01') & (oil_data['date'] < '2022-02-04')]\n",
    "\n",
    "#     return oil_data[['date', 'Oil Price']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e926d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oil_data = get_oil_price()\n",
    "# oil_data.to_csv (r'oil_data.csv')\n",
    "# oil_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_data = pd.read_csv('oil_data.csv', index_col=0)\n",
    "oil_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161700e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_period_after_war(data):\n",
    "    war_date = '2022-02-24'\n",
    "    return data[data['Transaction Date'] < war_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_period_after_war(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_oil_to_data(data, oil):\n",
    "    data['date_without_time'] = pd.to_datetime(data['Transaction Date']).dt.strftime('%Y-%m-%d')\n",
    "    oil['date_without_time'] = pd.to_datetime(oil['date']).dt.strftime('%Y-%m-%d')\n",
    "    data = data.merge(oil, left_on='date_without_time', right_on='date_without_time')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_oil_to_data(df, oil_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab59bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers_in_data(data):\n",
    "    quantiles = data.quantile(0.98)\n",
    "    data = data[(data['Amount'] < quantiles['Amount']) & (data['Property Size (sq.m)'] < quantiles['Property Size (sq.m)'])]\n",
    "    data = data[data['Property Sub Type'].isin([\"Commercial\", \"Flat\", \"Hotel Apartment\", \"Hotel Rooms\",  \"Office\", \"Residential\", \"Residential / Attached Villas\",\"Residential Flats\", \"Stacked Townhouses\", \"Villa\"])]\n",
    "    data = data.dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_outliers_in_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a43ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1436c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "def plot_matrix(data):\n",
    "    scatter_matrix(data[['Amount', 'Property Size (sq.m)']], figsize=(12, 8))\n",
    "    scatter_matrix(data[['Amount', 'Oil Price']], figsize=(12, 8))\n",
    "    return data['Property Size (sq.m)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d69cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_before_training(data, drop, renameColumns):\n",
    "    data = data.drop(drop, axis=1)\n",
    "    data = data.rename(columns=renameColumns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = preparing_data_before_training(\n",
    "    cleaned_df, \n",
    "    ['date_without_time', 'Transaction Date', 'date'],\n",
    "    {\n",
    "        'Property Size (sq.m)': 'Property_Size',\n",
    "        'Property Sub Type': 'Property_Sub_Type',\n",
    "        'Nearest Metro': 'Nearest_Metro',\n",
    "        'Nearest Mall': 'Nearest_Mall',\n",
    "        'Nearest Landmark': 'Nearest_Landmark',\n",
    "        'Oil Price': 'Oil_Price',\n",
    "        'Transaction Type': 'Transaction_Type',\n",
    "        \"Is Free Hold?\": 'Is_Free_Hold'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf #  + C(Nearest_Mall) + C(Nearest_Landmark) \n",
    "formula = 'Amount ~ C(Transaction_Type) + C(Usage) + C(Area) + C(Property_Sub_Type) + Property_Size + C(Nearest_Metro)+ Oil_Price + C(Is_Free_Hold)'\n",
    "sm_data = sm.add_constant(prepared_data)\n",
    "model = smf.ols(formula=formula, data=sm_data)\n",
    "results = model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c0cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05947fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(results, \"Property_Size\")\n",
    "fig.tight_layout(pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a117fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ols = results.get_prediction()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "x = prepared_data['Property_Size']\n",
    "y = prepared_data['Amount']\n",
    "\n",
    "ax.plot(x, y, \"o\", label=\"data\")\n",
    "ax.plot(x, results.fittedvalues, \"o\", label=\"OLS\")\n",
    "\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5659906",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\Corr}{Corr}$\n",
    "# Multicollinearity\n",
    "\n",
    "###### def:Multicollinearity is a linear relationship between explanatory variables. =>  $Corr(X_i, X_j) > 0$\n",
    "###### When we face multicollinearity:\n",
    "1. When regressors mean relatively the same (ex: height of a building and number of floors)\n",
    "2. Natural relationships between variables (ex: year of work and age)\n",
    "\n",
    "###### Consequences of multicollinearity:\n",
    "1. Estimates __remain unbiased__\n",
    "2. Increased standard errors\n",
    "3. Wide confidence intervals\n",
    "4. Insignificant coefficients\n",
    "5. Estimates become sensitive to slight changes\n",
    "\n",
    "###### How to detect multicollinearity:\n",
    "1. small changes lead to serious changes in estimates\n",
    "2. insignificant estimates due to big standard errors\n",
    "3. unexpected signs of estimates\n",
    "\n",
    "###### Indicators of multicollinearity:\n",
    "1. Strong covariance between regressors - calculate sample Var-Cov matrix\n",
    "2. Large value of VIF (Variance inflation factor) <br>\n",
    "$ VIF(x_j) = \\frac{1}{1 - R_j^2}$\n",
    ",where $ R_j^2$ is determination coefficient for regression <br>\n",
    "In VIF method, we pick each feature and regress it against all of the other features. Generally, a VIF above 5 indicates a high multicollinearity. <br>\n",
    "https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "\n",
    "###### Dealing with multicollinearity: (ЖЕЛАТЕЛЬНО ДОПОЛНИТЬ)\n",
    "1. Increase sample size\n",
    "2. Change functional form\n",
    "3. Impose some priory restrictions on parameters\n",
    "4. Dimension reduction / Principal component analysis (PCA)\n",
    "5. Ridge and LASSO regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352674cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import patsy\n",
    "\n",
    "def VIF(data, formula):  # from explanatory matrix and formula\n",
    "    \"\"\"\n",
    "    Calculates VIF coefficients for given dataframe. Returns DataFrame.\n",
    "    \n",
    "    Arguments:\n",
    "    data - DataFrame that have been used in regression\n",
    "    formula - Formula used in regression as a string, example: 'Amount ~ C(Transaction_Type) + C(Area)'\n",
    "    \n",
    "    Interpretation:\n",
    "    The variance inflation factor is a measure for the increase of the variance of the parameter estimates \n",
    "    if an additional variable, given by exog_idx is added to the linear regression. \n",
    "    It is a measure for multicollinearity of the design matrix, exog.\n",
    "\n",
    "    One recommendation is that if VIF is greater than 5, \n",
    "    then the explanatory variable given by exog_idx is highly collinear with the other explanatory variables,\n",
    "    and the parameter estimates will have large standard errors because of this.\n",
    "        \n",
    "    Sources:\n",
    "    https://en.wikipedia.org/wiki/Variance_inflation_factor\n",
    "    https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "    \n",
    "    \"\"\"\n",
    "    f = formula  \n",
    "    y, X = patsy.dmatrices(f, data, return_type='matrix')\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.design_info.column_names\n",
    "\n",
    "    vif_values = []\n",
    "    for i in range(len(X.design_info.column_names)):\n",
    "        vif_values.append(variance_inflation_factor(X, i))\n",
    "\n",
    "    vif_data['VIF'] = vif_values\n",
    "    return vif_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def VIF2(model, results):  # from model\n",
    "    \"\"\"\n",
    "    Calculates VIF coefficients for given model. Returns DataFrame.\n",
    "    \n",
    "    Arguments:\n",
    "    model - fitted regression model from Statsmodels\n",
    "    \n",
    "    Interpretation:\n",
    "    The variance inflation factor is a measure for the increase of the variance of the parameter estimates \n",
    "    if an additional variable, given by exog_idx is added to the linear regression. \n",
    "    It is a measure for multicollinearity of the design matrix, exog.\n",
    "\n",
    "    One recommendation is that if VIF is greater than 5, \n",
    "    then the explanatory variable given by exog_idx is highly collinear with the other explanatory variables,\n",
    "    and the parameter estimates will have large standard errors because of this.\n",
    "        \n",
    "    Sources:\n",
    "    https://en.wikipedia.org/wiki/Variance_inflation_factor\n",
    "    https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "    \n",
    "    \"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = model.exog_names\n",
    "\n",
    "    vif_values = []\n",
    "    for i in range(len(model.exog_names)):\n",
    "        vif_values.append(variance_inflation_factor(results.model.exog, i))\n",
    "\n",
    "    vif_data['VIF'] = vif_values\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF = VIF(sm_data, 'Amount ~ C(Transaction_Type) + C(Usage) + C(Area) + C(Property_Sub_Type) + Property_Size + Oil_Price + C(Is_Free_Hold)')  # sm_data, 'Amount ~ C(Transaction_Type) + C(Area) + C(Nearest_Landmark) + Oil_Price + C(Is_Free_Hold)'\n",
    "VIF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF.sort_values('VIF', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10020f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94852b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = pd.DataFrame(model.exog, columns=model.exog_names)['C(Property_Sub_Type)[T.Flat]']\n",
    "X_sub = pd.DataFrame(model.exog, columns=model.exog_names).drop(columns='C(Property_Sub_Type)[T.Flat]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9243977",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sub = sm.OLS(y_sub, X_sub)\n",
    "results_sub = model_sub.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0572b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b843db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245eb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.exog, columns=model.exog_names).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c91fc",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\Corr}{Corr}$\n",
    "# Heteroskedasticity\n",
    "\n",
    "###### def: The disturbances are homoscedastic if the variance of $\\epsilon_i$ is a constant $\\sigma ^{2}$; otherwise, they are heteroscedastic. =>  $ Var(\\epsilon_i) \\neq \\sigma^{2} $\n",
    "###### When we face heteroskedasticity:\n",
    "The most basic heteroskedastic example is household consumption. The variance in consumption increases with an increase in income—directly proportional. Because when the income is low, the variance in consumption is also low. Low-income people spend predominantly on necessary items and bills—less variance. In contrast, with the increase in income, people tend to buy luxurious items and develop a plethora of habits—less predictable.\n",
    "\n",
    "###### Consequences of heteroskedasticity:\n",
    "1. Standard errors (SE) of $\\hat{\\beta}$ calculated incorrectly => t-tests and F-tests are wrong\n",
    "2. OLS estimates are not BLUE anymore => efficiency loss\n",
    "3. Estimates are unbiased and consistent: $\\mathbb{E}(\\hat{\\beta}) = \\beta$,  ${\\hat{\\beta} \\to \\beta}$ as probability of estimates converge to probability of parameter\n",
    "\n",
    "###### How to detect heteroskedasticity:\n",
    "1. White test\n",
    "2. Goldfeld-Quandt test\n",
    "3. Glejser test\n",
    "4. Breusch-Pagan test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da4959",
   "metadata": {},
   "source": [
    "### White test\n",
    "\n",
    "H0: Homoskedasticity <br>\n",
    "H1: Heteroskedasticity <br>\n",
    "\n",
    "Step 1. Estimate the regression <br>\n",
    "example: $ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i $\n",
    "\n",
    "Step 2. Save residuals $ \\hat{\\epsilon_i} $ <br>\n",
    "Step 3. Build the auxiliary model <br>\n",
    "Regress squares of saved $ \\hat{\\epsilon_i} $ __on__ regressors $x_{1i}$ and $x_{2i}$, their squares and their product <br>\n",
    "$ \\hat{\\epsilon_i}^2 = \\alpha_0 + \\alpha_1 x_{1i} + \\alpha_2 x_{2i} + \\alpha_3 x_{1i}^2 + \\alpha_4 x_{2i}^2 + \\alpha_5 x_{1i} x_{2i} + \\epsilon_i $ <br>\n",
    "Step 4. Calculate $R^2$ for auxiliary model <br>\n",
    "Step 5. Calculate test statistic <br>\n",
    "$ W = n R^2 \\sim \\chi_{k-1}^2 $, where k is the number of estimated coefficients in auxiliary regression <br>\n",
    "\n",
    "#### Interpretation\n",
    "High R^2 means that we have the dependance between the variance of the disturbance term and the regressors\n",
    "\n",
    "#### Suitable for large samples and doesn't imply the normality of residuals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Долго считается, не дождался\n",
    "def white_test(residuals, exog):\n",
    "    #perform White's test\n",
    "    white_test = het_white(residuals,  exog)\n",
    "    #define labels to use for output of White's test\n",
    "    labels = ['Test Statistic', 'Test Statistic p-value', 'F-Statistic', 'F-Test p-value']\n",
    "    #print results of White's test\n",
    "    output = dict(zip(labels, white_test))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04e841",
   "metadata": {},
   "source": [
    "### Goldfeld-Quandt test\n",
    "\n",
    "H0: Homoskedasticity $ \\sigma_i^2 = \\sigma^2$,  $ \\forall_1i = 1, ..., n $<br>\n",
    "H1: Heteroskedasticity  $ \\sigma_i^2 \\sim x_{ji}$ for some $ x_j $, variance is proportional to some regressor $ x_j $<br>\n",
    "\n",
    "Step 1. Sort data by variable $ x_j $, that we suspect bring heteroskedasticity\n",
    "\n",
    "Step 2. Split sample into 3 parts.\n",
    "\n",
    "Step 3. Estimate regressions on the subsamples for first $ n_1 $ and last $ n_2 $ observations and calculate $ RSS_1$ and $ RSS_2$\n",
    "\n",
    "\n",
    "Step 4. Calculate test statistic: <br>\n",
    "$ F = \\frac{\\frac{RSS_2}{n_2 - k}}{\\frac{RSS_1}{n_1 - k}}\n",
    "$\n",
    "\n",
    "Step 5. Under H0 <br>\n",
    "$ F \\sim F_{n2-k, n1-k} $\n",
    "\n",
    "\n",
    "#### Suitable for small samples and doesn't imply the normality of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldfeldquandt_test(results, model): # from model\n",
    "    \"\"\"\n",
    "    Calculates F-Stat and p-value for given fitted model. Returns Dictionary.\n",
    "    \n",
    "    Arguments:\n",
    "    results - fitted regression model from Statsmodels\n",
    "    model - unfitted model from Statsmodel\n",
    "    idx - index of a column as a int, by which explanatory variables are sorted\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = ['F-Statistic', 'F-Test p-value', 'Ordering used']\n",
    "    values = []\n",
    "    for i in range(len(model.exog_names)):\n",
    "        values.append(het_goldfeldquandt(results.model.endog, results.model.exog, idx=i))\n",
    "\n",
    "    output = pd.DataFrame(values, columns = labels)\n",
    "    output['feature'] = model.exog_names\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7494ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#het = goldfeldquandt_test(results, model)\n",
    "#het"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407c207b",
   "metadata": {},
   "source": [
    "### Glejser test\n",
    "\n",
    "H0: Homoskedasticity $ \\sigma_i^2 = \\sigma^2$,  $ \\forall_1i = 1, ..., n $<br>\n",
    "H1: Heteroskedasticity  $ \\sigma_i^2 \\sim x_{j}^\\gamma $ for $ \\gamma = 1, 1/2 $ or $ -1 $ <br>\n",
    "Glejser test assumes that we can have the dependance between the $ \\sigma_i^2 $ and some powers of $ x_{j} $, so the $ \\sigma_i^2 $ is proportional to $ x_{j}^\\gamma $, where $ \\gamma $ can be 1, 1/2 or -1\n",
    "\n",
    "\n",
    "Step 1. Estimate regression on the whole sample <br>\n",
    "\n",
    "Step 2. Predict residuals $ \\epsilon_i $  <br>\n",
    "\n",
    "Step 3. Take absolute values of predicted $ \\epsilon_i $ and regress the folowing: <br>\n",
    "$ |\\epsilon_i| = \\alpha + \\beta x_i + \\mu_i $, for  i = 1, ..., n <br>\n",
    "$ |\\epsilon_i| = \\alpha + \\beta \\sqrt{x_i} + \\mu_i $, for  i = 1, ..., n <br>\n",
    "$ |\\epsilon_i| = \\alpha + \\beta \\frac{1}{x_i} + \\mu_i $, for  i = 1, ..., n <br>\n",
    "\n",
    "\n",
    "Step 4. Use t-test to check significance of all estimated $ \\beta $'s <br>\n",
    "if any of Betas is significant then we have heteroskedasticity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В лекциях/вики среди прочего есть модель с делением единицы на регрессор х \n",
    "# (x_div), что ведет к ошибкам если есть хотя бы один 0.\n",
    "# НЕ СМОГ КОНВЕРТИРОВАТЬ ТИП ДАННЫХ В ТАБЛИЦЕ СТАТСМОДЕЛС(\n",
    "\n",
    "\"\"\"\n",
    "In this example, we load the \"fair\" dataset from the statsmodels library and fit a linear regression \n",
    "model to it using the OLS function. We then create a new DataFrame for the Glejser test and calculate \n",
    "the square root of the absolute residuals and the squared value of the \"educ\" variable. We fit another \n",
    "linear regression model to this DataFrame using the OLS function and print the summary of the results.\n",
    "\n",
    "Note that in the Glejser test, we regress the square root of the absolute residuals on the independent \n",
    "variables, which in this case are the \"educ\" variable and its squared value. The null hypothesis is that \n",
    "the variance of the residuals is constant across all levels of the independent variables.\n",
    "\"\"\"\n",
    "import itertools\n",
    "\n",
    "# Run the Glejser test\n",
    "\n",
    "def glejser(results, model):\n",
    "    output = pd.DataFrame(columns=['regressor_type', 'coef', 'std_err', 't_stat', 'P_value>|t|', '[0.025', '0.975]'])\n",
    "    \n",
    "    for j in range(len(model.exog_names)):\n",
    "        \n",
    "        # Create regressors and regressant for auxiliary regressions\n",
    "        y = abs(results.resid)\n",
    "        X_reg = model.exog[:, j]\n",
    "        X_root = model.exog[:, j] ** 0.5\n",
    "        X_sqr = model.exog[:, j] ** 2\n",
    "        iterable = [X_reg, X_root, X_sqr]\n",
    "\n",
    "        # Run the Glejser test and append it in dataframe (only Beta coef's are saved)\n",
    "\n",
    "        for i in iterable:\n",
    "            glejser_model = sm.OLS(y, sm.add_constant(i))\n",
    "            glejser_results = glejser_model.fit()\n",
    "            a = pd.DataFrame(glejser_results.summary().tables[1])\n",
    "            a.rename(columns={a.columns[0]: 'regressor_type',\n",
    "                          a.columns[1]: 'coef',\n",
    "                          a.columns[2]: 'std_err',\n",
    "                          a.columns[3]: 't_stat',\n",
    "                          a.columns[4]: 'P_value>|t|',\n",
    "                          a.columns[5]: '[0.025',\n",
    "                          a.columns[6]: '0.975]',\n",
    "                         }, inplace=True)\n",
    "            a.drop(index=[0, 1], inplace=True)\n",
    "            a['feature'] = model.exog_names[j]\n",
    "            output = pd.concat([output, a], axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    # Create column with name of used auxiliary regressor\n",
    "\n",
    "    num_cycle = itertools.cycle([1, 2, 3])\n",
    "    output['regressor_type'] = [next(num_cycle) for num in range(len(output))]\n",
    "    output['regressor_type'].replace([1, 2, 3], ['X_reg', 'X_root', 'X_sqr'], inplace=True)\n",
    "    output.reset_index(drop=True, inplace=True)\n",
    "    #output = output.infer_objects()\n",
    "    #output = output.astype(dtype={'regressor_type': str, 'coef': float, 'std_err': float, 't_stat': float, 'P_value>|t|': float, '[0.025': float, '0.975]': float, 'feature': str})\n",
    "    #output[['coef', 'std_err', 't_stat', 'P_value>|t|', '[0.025', '0.975]']] = output[['coef', 'std_err', 't_stat', 'P_value>|t|', '[0.025', '0.975]']].astype(float)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glejser_test = glejser(results, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32153be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def settype(t):\n",
    "    t.set_datatype(float)\n",
    "    return t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glejser_test['t_stat'].apply(settype).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c588d17",
   "metadata": {},
   "source": [
    "### Breusch-Pagan test\n",
    "\n",
    "H0: Homoskedasticity $ \\sigma_i^2 = \\sigma^2$,  $ \\forall_1i = 1, ..., n $<br>\n",
    "H1: Heteroskedasticity  $ \\sigma_i^2 \\sim f(\\alpha_0 + \\alpha_1 z_{1i} + ... + \\alpha_r z_{ri}) $ <br>\n",
    "\n",
    "Test of dependance of variance on other variables z\n",
    "\n",
    "\n",
    "Step 1. Estimate regression <br>\n",
    "\n",
    "Step 2. Predict residuals $ \\epsilon_i $ and calculate RSS  <br>\n",
    "\n",
    "Step 3. Find estimate for variance: <br>\n",
    "$ \\hat{\\sigma_u^2} = \\frac{RSS}{n} $ <br>\n",
    "\n",
    "Step 4. Estimate regression: <br>\n",
    "$ \\epsilon_i^2 = \\gamma_0 + \\gamma_1 z_1 + ... + \\gamma_r z_r $\n",
    "\n",
    "Step 5. Calculate $ ESS_0 $ from regression in Step 4 <br>\n",
    "\n",
    "Step 6. Calculate test statistic <br>\n",
    "\n",
    "$ BP = \\frac{ESS_0}{2\\hat{\\sigma^4}} \\sim \\chi_r^2 $ , where $ r $ is a number of variables z <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa931c0",
   "metadata": {},
   "source": [
    "#### If we have homoskedasticity we can estimate regression easy <br>\n",
    "$ Var(\\hat{\\beta}) = (X^\\prime X)^{-1} X^\\prime \\Omega X (X^\\prime X)^{-1} $, <br>\n",
    "where $ \\Omega $ is the covariance matrix.\n",
    "In case of homoskedasticity $ \\Omega $: <br>\n",
    "$ \\Omega = \\begin{pmatrix} \\sigma^2 & ... & 0\\\\ ... & ... & ...\\\\ 0 & ... & \\sigma^2 \\end{pmatrix}  $, <br>\n",
    "hence equation simplifies to: <br>\n",
    "$ Var(\\hat{\\beta}) = \\sigma^2 (X^\\prime X)^{-1}  $ <br>\n",
    "\n",
    "#### But in case of heteroskedasticity $ \\Omega $ isn't diagonal: <br>\n",
    "hence equation will look like this: <br>\n",
    "$ Var(\\hat{\\beta}) = (X^\\prime X)^{-1} X^\\prime (\\sum_{i=1}^n \\sigma_i^2 x_i x_i^\\prime) X (X^\\prime X)^{-1} $ <br>\n",
    "\n",
    "##### 1. White's estimator (heteroskedasticity-consistent estimator, HCE): <br>\n",
    "$ Var(\\hat{\\beta}) = (X^\\prime X)^{-1} X^\\prime (\\sum_{i=1}^n \\epsilon_i^2 x_i x_i^\\prime) X (X^\\prime X)^{-1} $ <br>\n",
    "\n",
    "##### 2. Generalized Least Squares (GLS): <br>\n",
    "##### 3. Feasible Generalized Least Squares (FGLS): <br>\n",
    "##### 4. Weighted Least Squares: <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba2097",
   "metadata": {},
   "source": [
    "### Hypotheses:\n",
    "1. Regions closer to the waterline add additional value to property cost - could be visualized with map\n",
    "2. Price for property is affected by fluctuations in prices of oil\n",
    "3. Property without metro station around cost less.\n",
    "4. Commercial property cost more than residential\n",
    "5. Increase in area positively influence propery price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732b3cc",
   "metadata": {},
   "source": [
    "### Need to add:\n",
    "#### A couple more graphs to illustrate categoriacal features (barcharts, boxplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe3e40",
   "metadata": {},
   "source": [
    "We have chosen OLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe366f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 = Model correctly spicified\n",
    "# H1 = Model misspecified\n",
    "\n",
    "ramsey_test = linear_reset(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try = sm_data\n",
    "data_try['Price_Sq_m'] = data_try['Amount'] / data_try['Property_Size']\n",
    "\n",
    "formula_try = 'Price_Sq_m ~ C(Transaction_Type) + C(Usage) + C(Area) + Property_Size + C(Nearest_Metro) + Oil_Price + C(Is_Free_Hold)'\n",
    "\n",
    "model_try = smf.ols(formula=formula_try, data=data_try)\n",
    "results_try = model_try.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_try.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017832d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_try.select_dtypes(\"number\").drop(columns=['const', 'Price_Sq_m']).corr()\n",
    "sns.heatmap(corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba8beb",
   "metadata": {},
   "source": [
    "#### 1A <br>\n",
    "a - Берем нашу первую модель, можно добавить геопозицию <br>\n",
    "б - Удобно протестировать (1. Что с площадью растет и цена 2. Что с приближением к воде растет цена) <br>\n",
    "с - Можно трансформировать Y сказав, что будем прогнозировать Цена/Площадь <br>\n",
    "или просто логарифмировать что-то, возвести в квадрат => сравнить модели F тестом <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 1Б <br>\n",
    "д. Мне кажется можно описать модель с предыдущего шага. Описание мисспесификаций предположительных, __пока без тестов__. <br>\n",
    "Можно трансформировать Y сказав, что будем прогнозировать Цена/Площадь <br>\n",
    "или просто логарифмировать что-то, возвести в квадрат => сравнить модели F тестом <br>\n",
    "\n",
    "е. Тесты на спецификацию: <br>\n",
    "Перебор разных переменных, <br>\n",
    "Еще трансформации, <br>\n",
    "Тест Рамзей-РЕСЕТ, CHOW TEST (structural break) <br>\n",
    "\n",
    "ф. Предфинальная модель - нормально специфицирована - предварительная интерпритация результатов <br>\n",
    "\n",
    "#### 2 <br>\n",
    "а - проверить мультиколлиниарность - практически сделал - VIF - после можно отбросить лишние параметры <br>\n",
    "б - Гетероскедастичность - Тесты есть, можно попробовать избавиться <br>\n",
    "с - Эндодженеити - Описание, что потенциально может быть эндодженос <br>\n",
    "д - ФИНАЛЬНАЯ МОДЕЛЬ, ИНТЕРПРИТАЦИЯ, R^2, Подтвердились ли гипотезы? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c07ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
